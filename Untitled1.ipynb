{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cf0d228-5e1c-445b-9281-e5a965352335",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'LSTM_Prep'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11592/3595986622.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM_Prep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'LSTM_Prep'"
     ]
    }
   ],
   "source": [
    "# Importation\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import LSTM_Prep\n",
    "\n",
    "# Data\n",
    "dat = pd.read_csv('forex.csv')\n",
    "\n",
    "split = 0.8\n",
    "sequence_length = 60\n",
    "\n",
    "data_prep = LSTM_Prep.Data_Prep(dataset = dat)\n",
    "rnn_df, validation_df = data_prep.preprocess_rnn(date_colname = 'date', numeric_colname = 'perc', pred_set_timesteps = 60)\n",
    "\n",
    "\n",
    "series_prep = LSTM_Prep.Series_Prep(rnn_df =  rnn_df, numeric_colname = 'perc')\n",
    "window, X_min, X_max = series_prep.make_window(sequence_length = sequence_length, \n",
    "                                               train_test_split = split, \n",
    "                                               return_original_x = True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = series_prep.reshape_window(window, train_test_split = split)\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "#                 Building the LSTM\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import ReduceLROnPlateau #Learning rate scheduler for when we reach plateaus\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100)\n",
    "\n",
    "# Reset model if we want to re-train with different splits\n",
    "def reset_weights(model):\n",
    "    import keras.backend as K\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'): \n",
    "            layer.kernel.initializer.run(session=session)\n",
    "        if hasattr(layer, 'bias_initializer'):\n",
    "            layer.bias.initializer.run(session=session)  \n",
    "\n",
    "\n",
    "# Epochs and validation split\n",
    "EPOCHS = 201\n",
    "validation = 0.05\n",
    "\n",
    "# Instantiate the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer.... the input shape is (Sample, seq_len-1, 1)\n",
    "model.add(LSTM(\n",
    "        input_shape = (sequence_length-1, 1), return_sequences = True,\n",
    "        units = 100))\n",
    "\n",
    "# Add the second layer.... the input shape is (Sample, seq_len-1, 1)\n",
    "model.add(LSTM(\n",
    "        input_shape = (sequence_length-1, 1), \n",
    "        units = 100))\n",
    "\n",
    "# Add the output layer, simply one unit\n",
    "model.add(Dense(\n",
    "        units = 1,\n",
    "        activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "\n",
    "# History object for plotting our model loss by epoch\n",
    "history = model.fit(X_train, y_train, epochs = EPOCHS, validation_split = validation,\n",
    "          callbacks = [rlrop])\n",
    "# Loss History\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "#              Predicting the future\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "# Creating our future object\n",
    "future = LSTM_Prep.Predict_Future(X_test  = X_test, validation_df = validation_df, lstm_model = model)\n",
    "# Checking its accuracy on our training set\n",
    "future.predicted_vs_actual(X_min = X_min, X_max = X_max, numeric_colname = 'perc')\n",
    "# Predicting 'x' timesteps out\n",
    "future.predict_future(X_min = X_min, X_max = X_max, numeric_colname = 'perc', timesteps_to_predict = 15, return_future = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
